# AEC (Auto Encoder & Classifier)

对于这个任务，我们还尝试了使用自编码器与分类器的结合，进行半监督学习。AEC模型结合了自编码器的无监督学习能力和分类器的有监督分类能力，能够同时利用有标签和无标签数据进行训练。

## 模型架构

AEC模型由三个主要部分组成：

### 1. 编码器 (Encoder)
- 输入维度：22个特征
- 隐藏层结构：[14, 8, 6]
- 每层包含：线性层 + 批归一化 + LeakyReLU激活函数 + Dropout(0.2)
- 输出：潜在表示，维度为5

### 2. 解码器 (Decoder)
- 输入：潜在表示，维度为5
- 隐藏层结构：[6, 8, 14]（编码器的反向结构）
- 每层包含：线性层 + 批归一化 + LeakyReLU激活函数 + Dropout(0.2)
- 输出：重构数据，维度为22

### 3. 分类器 (Classifier)
- 输入：潜在表示，维度为5
- 隐藏层结构：[8, 6]
- 每层包含：线性层 + 批归一化 + LeakyReLU激活函数 + Dropout(0.2)
- 输出：2个类别的logits

模型总参数量适中，针对小数据集设计，避免过拟合。

## 训练过程

训练过程分为两个阶段，实现了半监督学习：

### 阶段1：有标签数据训练
- 使用有标签数据同时训练编码器、解码器和分类器
- 损失函数：总损失 = α × 重构损失 + β × 分类损失
  - 重构损失：均方误差 (MSE)
  - 分类损失：交叉熵损失 (Cross-Entropy)
  - 默认权重：α=1.0, β=1.0
- 添加L1正则化项（λ=0.01）防止过拟合

### 阶段2：无标签数据训练
- 使用无标签数据只训练编码器和解码器
- 仅计算重构损失，不使用分类损失
- 同样添加L1正则化项

### 训练细节
- 优化器：Adam (学习率=0.001)
- 训练轮数：200
- 批次大小：32
- 设备：自动选择GPU或CPU

## 数据处理

### 数据集结构
- 训练集：包含特征和标签
- 测试集：仅包含特征
- 特征维度：22
- 标签：二分类（0/1）

### 数据预处理
- 使用StandardScaler进行特征标准化
- 训练集拟合scaler，测试集使用相同的scaler
- 创建PyTorch Dataset和DataLoader进行批量处理

## 效果评估

### 训练监控
- 记录训练过程中的总损失、重构损失、分类损失和准确率
- 记录测试集上的重构损失，监控过拟合
- 绘制训练曲线，可视化训练过程

### 模型评估
- 在训练集上评估模型性能
- 计算准确率、各类损失值
- 使用t-SNE降维可视化潜在空间
- 对比真实标签和预测标签的分布

### 结果分析
在有标签数据上的效果，与直接训练分类器的效果基本相同。

在无标签数据上的效果，与直接使用自编码器的效果基本相同。

## 模型优势

1. **半监督学习能力**：能够同时利用有标签和无标签数据进行训练
2. **特征学习**：通过自编码器学习数据的低维表示，有助于分类任务
3. **防止过拟合**：通过Dropout、批归一化和L1正则化等技术防止过拟合
4. **端到端训练**：编码器、解码器和分类器可以联合优化

## 应用场景

AEC模型特别适用于以下场景：
- 有标签数据有限，但有大量无标签数据可用
- 需要同时进行特征降维和分类任务
- 希望模型具有良好的泛化能力

## 文件结构

- `model.py`：包含AEC模型的定义
- `train_aec.py`：AEC模型的训练脚本
- `preprocess.py`：数据预处理相关代码
