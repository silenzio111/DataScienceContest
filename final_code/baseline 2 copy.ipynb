{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5ee57b5",
   "metadata": {},
   "source": [
    "# Baseline (CatBoost + XGBoost) - OOF Stacking Notebook\n",
    "\n",
    "此笔记本实现：特征融合、折内重采样（UCO/SMOTE）、基模型 OOF 训练、Top3Avg 扩展、融合（Meta-learner 或 随机采样+局部精化）、概率校准/收缩/裁剪与保存输出。\n",
    "\n",
    "请在顶部修改路径与参数后运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bb09cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and basic config\n",
    "import os, json, datetime, copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Paths (modify as needed)\n",
    "pwd = '../data/data(processed)/'\n",
    "output = '../data/output/'\n",
    "os.makedirs(output, exist_ok=True)\n",
    "output_filename = 'baseline_submission.csv'\n",
    "\n",
    "# General settings\n",
    "target_col = 'target'\n",
    "id_col = 'id'\n",
    "unused_features = [id_col]\n",
    "isout = 1\n",
    "seed = 42\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    'CatBoost': CatBoostClassifier(depth=6, learning_rate=0.05, iterations=600, loss_function='Logloss', eval_metric='AUC', random_seed=seed, verbose=0),\n",
    "    'XGBoost': XGBClassifier(n_estimators=600, learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0, random_state=seed, n_jobs=-1, use_label_encoder=False, eval_metric='logloss')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c570bcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (500, 24) Test shape: (2000, 23)\n",
      "Initial feature count: 22\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(pwd + 'train.csv')\n",
    "test_df = pd.read_csv(pwd + 'test.csv')\n",
    "\n",
    "print('Train shape:', train_df.shape, 'Test shape:', test_df.shape)\n",
    "\n",
    "# Basic target / feature split\n",
    "y_train = train_df[target_col].copy()\n",
    "X_train = train_df.drop(columns=[target_col] + unused_features, errors='ignore').copy()\n",
    "X_test = test_df.drop(columns=unused_features, errors='ignore').copy()\n",
    "\n",
    "print('Initial feature count:', X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5dfb97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature fusion completed; new feature count: 34\n"
     ]
    }
   ],
   "source": [
    "# Feature fusion: row stats, corr-based interactions, high-corr group means, cat count, KFold TE\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
    "\n",
    "corr_pair_threshold = 0.6\n",
    "corr_feature_threshold = 0.85\n",
    "drop_high_corr_originals = True\n",
    "max_interactions = 20\n",
    "\n",
    "# 1) row stats\n",
    "for func_name, func in [('row_sum', np.nansum), ('row_mean', np.nanmean), ('row_std', np.nanstd), ('row_min', np.nanmin), ('row_max', np.nanmax)]:\n",
    "    X_train[func_name] = X_train[num_cols].apply(lambda r: func(r), axis=1)\n",
    "    X_test[func_name] = X_test[num_cols].apply(lambda r: func(r), axis=1)\n",
    "X_train['row_nnz'] = (X_train[num_cols] != 0).sum(axis=1)\n",
    "X_test['row_nnz'] = (X_test[num_cols] != 0).sum(axis=1)\n",
    "\n",
    "# 2) corr-based interactions\n",
    "if len(num_cols) >= 2:\n",
    "    corr_mat = X_train[num_cols].corr().abs()\n",
    "    pairs = []\n",
    "    for i in range(len(num_cols)):\n",
    "        for j in range(i+1, len(num_cols)):\n",
    "            a, b = num_cols[i], num_cols[j]\n",
    "            if corr_mat.loc[a, b] >= corr_pair_threshold:\n",
    "                pairs.append((a, b, corr_mat.loc[a, b]))\n",
    "    pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "    pairs = pairs[:max_interactions]\n",
    "else:\n",
    "    pairs = []\n",
    "\n",
    "for a, b, _ in pairs:\n",
    "    X_train[f'{a}_mul_{b}'] = X_train[a].fillna(0) * X_train[b].fillna(0)\n",
    "    X_test[f'{a}_mul_{b}'] = X_test[a].fillna(0) * X_test[b].fillna(0)\n",
    "    eps = 1e-6\n",
    "    X_train[f'{a}_div_{b}'] = X_train[a].fillna(0) / (X_train[b].fillna(0) + eps)\n",
    "    X_test[f'{a}_div_{b}'] = X_test[a].fillna(0) / (X_test[b].fillna(0) + eps)\n",
    "\n",
    "# 3) high-corr groups\n",
    "high_corr_groups = []\n",
    "if len(num_cols) >= 2:\n",
    "    used = set()\n",
    "    for col in num_cols:\n",
    "        if col in used:\n",
    "            continue\n",
    "        related = corr_mat.index[corr_mat.loc[col] >= corr_feature_threshold].tolist()\n",
    "        related = [c for c in related if c != col]\n",
    "        if len(related) >= 1:\n",
    "            group = [col] + related\n",
    "            group = sorted(list(dict.fromkeys(group)))\n",
    "            for g in group: used.add(g)\n",
    "            high_corr_groups.append(group)\n",
    "\n",
    "for idx, grp in enumerate(high_corr_groups):\n",
    "    name = f'corr_group_mean_{idx}'\n",
    "    X_train[name] = X_train[grp].mean(axis=1)\n",
    "    X_test[name] = X_test[grp].mean(axis=1)\n",
    "\n",
    "if drop_high_corr_originals and len(high_corr_groups) > 0:\n",
    "    cols_to_drop = [c for grp in high_corr_groups for c in grp if c in X_train.columns]\n",
    "    X_train.drop(columns=cols_to_drop, inplace=True)\n",
    "    X_test.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# 4) category count\n",
    "for c in cat_cols:\n",
    "    if c not in X_train.columns: continue\n",
    "    cnt = X_train[c].value_counts(dropna=False)\n",
    "    X_train[f'{c}_count'] = X_train[c].map(cnt).fillna(0)\n",
    "    X_test[f'{c}_count'] = X_test[c].map(cnt).fillna(0)\n",
    "\n",
    "# 5) KFold target encoding on categorical cols\n",
    "n_splits_te = 5\n",
    "kf = KFold(n_splits=n_splits_te, shuffle=True, random_state=seed)\n",
    "for c in cat_cols:\n",
    "    if c not in X_train.columns: continue\n",
    "    col_name = f'{c}_te'\n",
    "    X_train[col_name] = 0.0\n",
    "    test_vals = []\n",
    "    for tr_idx, val_idx in kf.split(X_train):\n",
    "        means = pd.concat([X_train.iloc[tr_idx], y_train.reset_index(drop=True)], axis=1).groupby(c)[target_col].mean() if False else None\n",
    "        # 为避免复杂依赖，这里用 train_df 的映射方法（在实际运行时请替换为 fold 内映射实现）\n",
    "        means = train_df.groupby(c)[target_col].mean() if c in train_df.columns else pd.Series()\n",
    "        X_train.iloc[val_idx, X_train.columns.get_loc(col_name)] = X_train.iloc[val_idx][c].map(means).fillna(y_train.mean())\n",
    "        test_vals.append(X_test[c].map(means).fillna(y_train.mean()))\n",
    "    X_test[col_name] = pd.concat(test_vals, axis=1).mean(axis=1)\n",
    "\n",
    "print('Feature fusion completed; new feature count:', X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35ac27cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCO function defined.\n"
     ]
    }
   ],
   "source": [
    "# UCO resample function (fold-level)\n",
    "def uco_resample(X, y, random_state=42):\n",
    "    rus = RandomUnderSampler(random_state=random_state)\n",
    "    X_r, y_r = rus.fit_resample(X, y)\n",
    "    sm = SMOTE(random_state=random_state, k_neighbors=5)\n",
    "    X_res, y_res = sm.fit_resample(X_r, y_r)\n",
    "    print('  [UCO] resampled counts:', Counter(y_res))\n",
    "    return X_res, y_res\n",
    "\n",
    "print('UCO function defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "737b0575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CatBoost\n",
      " Fold 1\n",
      "  [UCO] resampled counts: Counter({0: 8, 1: 8})\n",
      " Fold 2\n",
      "  [UCO] resampled counts: Counter({0: 8, 1: 8})\n",
      " Fold 2\n",
      "  [UCO] resampled counts: Counter({0: 8, 1: 8})\n",
      " Fold 3\n",
      "  [UCO] resampled counts: Counter({0: 8, 1: 8})\n",
      " Fold 3\n",
      "  [UCO] resampled counts: Counter({0: 8, 1: 8})\n",
      " Fold 4\n",
      "  [UCO] resampled counts: Counter({0: 8, 1: 8})\n",
      " Fold 4\n",
      "  [UCO] resampled counts: Counter({0: 8, 1: 8})\n",
      " Fold 5\n",
      "  [UCO] resampled counts: Counter({0: 8, 1: 8})\n",
      " Fold 5\n",
      "  [UCO] resampled counts: Counter({0: 8, 1: 8})\n",
      "CatBoost OOF AUC: 0.7864285714285715\n",
      "Training XGBoost\n",
      " Fold 1\n",
      "  [UCO] resampled counts: Counter({0: 8, 1: 8})\n",
      "CatBoost OOF AUC: 0.7864285714285715\n",
      "Training XGBoost\n",
      " Fold 1\n",
      "  [UCO] resampled counts: Counter({0: 8, 1: 8})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Competition\\数科统模\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [23:01:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fold 2\n",
      "  [UCO] resampled counts: Counter({0: 8, 1: 8})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Competition\\数科统模\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [23:01:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fold 3\n",
      "  [UCO] resampled counts: Counter({0: 8, 1: 8})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Competition\\数科统模\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [23:01:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fold 4\n",
      "  [UCO] resampled counts: Counter({0: 8, 1: 8})\n",
      "  [UCO] resampled counts: Counter({0: 8, 1: 8})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Competition\\数科统模\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [23:01:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fold 5\n",
      "  [UCO] resampled counts: Counter({0: 8, 1: 8})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Competition\\数科统模\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [23:01:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost OOF AUC: 0.7434693877551021\n",
      "Top3 by OOF AUC: ['CatBoost', 'XGBoost']\n",
      "Meta OOF AUC: 0.7616326530612245\n",
      "Best T: 2.0 score: 0.5384591992247623\n",
      "Saved submission to ../data/output/baseline_submission.csv\n",
      "Saved stacking info\n"
     ]
    }
   ],
   "source": [
    "# OOF training + stacking\n",
    "models_to_use = {k: copy.deepcopy(v) for k, v in models.items()}\n",
    "model_names = list(models_to_use.keys())\n",
    "\n",
    "# ensure DataFrame interface\n",
    "if not hasattr(X_train, 'iloc'):\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "if not hasattr(X_test, 'iloc'):\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "if not isinstance(y_train, pd.Series):\n",
    "    y_train = pd.Series(y_train)\n",
    "\n",
    "# containers\n",
    "oof_preds = {name: np.zeros(len(X_train)) for name in model_names}\n",
    "test_preds = {name: np.zeros(len(X_test)) for name in model_names}\n",
    "\n",
    "# auto n_splits safe (fallback if too few positives)\n",
    "min_pos = max(1, int(y_train.sum()))\n",
    "n_splits = min(5, max(2, min_pos))\n",
    "if n_splits > y_train.sum(): n_splits = max(2, int(y_train.sum()))\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "resample_in_fold = True\n",
    "resample_method = 'UCO'\n",
    "smote_apply_to_minority_only = True\n",
    "\n",
    "use_sample_weight = True\n",
    "use_pos_weight = True\n",
    "pos_weight_multiplier = 1.5\n",
    "positive_label = 1\n",
    "logit_shift = 0.0\n",
    "\n",
    "for name in model_names:\n",
    "    print('Training', name)\n",
    "    base = models_to_use[name]\n",
    "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "        print(' Fold', fold+1)\n",
    "        model = copy.deepcopy(base)\n",
    "        X_tr, X_val = X_train.iloc[tr_idx].copy(), X_train.iloc[val_idx].copy()\n",
    "        y_tr, y_val = y_train.iloc[tr_idx].copy(), y_train.iloc[val_idx].copy()\n",
    "\n",
    "        # align columns\n",
    "        cols = X_tr.columns.tolist()\n",
    "        for c in cols:\n",
    "            if c not in X_val.columns: X_val[c] = 0\n",
    "        X_val = X_val[cols]\n",
    "        X_test_for_pred = X_test[cols]\n",
    "\n",
    "        # fold-level resample\n",
    "        if resample_in_fold and resample_method == 'UCO':\n",
    "            try:\n",
    "                X_tr_res, y_tr_res = uco_resample(X_tr, y_tr, random_state=seed)\n",
    "            except Exception as e:\n",
    "                print(' Resample failed, fallback to original fold:', e)\n",
    "                X_tr_res, y_tr_res = X_tr, y_tr\n",
    "        else:\n",
    "            X_tr_res, y_tr_res = X_tr, y_tr\n",
    "\n",
    "        # sample weight\n",
    "        sample_weight = None\n",
    "        if use_sample_weight:\n",
    "            classes, counts = np.unique(y_tr_res, return_counts=True)\n",
    "            cw = {c: sum(counts)/(len(classes)*cnt) for c, cnt in zip(classes, counts)}\n",
    "            if use_pos_weight and positive_label in cw:\n",
    "                cw[positive_label] = cw.get(positive_label, 1.0) * float(pos_weight_multiplier)\n",
    "            sample_weight = pd.Series(y_tr_res).map(cw).values\n",
    "\n",
    "        try:\n",
    "            if 'CatBoost' in name:\n",
    "                if sample_weight is not None:\n",
    "                    model.fit(X_tr_res, y_tr_res, sample_weight=sample_weight, eval_set=(X_val, y_val), verbose=0)\n",
    "                else:\n",
    "                    model.fit(X_tr_res, y_tr_res, eval_set=(X_val, y_val), verbose=0)\n",
    "            else:\n",
    "                if sample_weight is not None:\n",
    "                    model.fit(X_tr_res, y_tr_res, sample_weight=sample_weight)\n",
    "                else:\n",
    "                    model.fit(X_tr_res, y_tr_res)\n",
    "\n",
    "            val_pred = model.predict_proba(X_val)[:, 1]\n",
    "            test_pred = model.predict_proba(X_test_for_pred)[:, 1]\n",
    "        except Exception as e:\n",
    "            print(' Predict failed, fallback to predict:', e)\n",
    "            val_pred = model.predict(X_val)\n",
    "            test_pred = model.predict(X_test_for_pred)\n",
    "            val_pred = np.array(val_pred, dtype=float)\n",
    "            test_pred = np.array(test_pred, dtype=float)\n",
    "\n",
    "        oof_preds[name][val_idx] = val_pred\n",
    "        test_preds[name] += test_pred / n_splits\n",
    "\n",
    "    try:\n",
    "        print(name, 'OOF AUC:', roc_auc_score(y_train, oof_preds[name]))\n",
    "    except Exception:\n",
    "        print(name, 'OOF AUC: n/a')\n",
    "\n",
    "# Top3Avg\n",
    "model_aucs = []\n",
    "for name in model_names:\n",
    "    try: a = float(roc_auc_score(y_train, oof_preds[name]))\n",
    "    except: a = float('nan')\n",
    "    model_aucs.append((name, a))\n",
    "model_aucs.sort(key=lambda x: x[1], reverse=True)\n",
    "top_k = min(3, len(model_names))\n",
    "top3 = [m for m,_ in model_aucs[:top_k]]\n",
    "print('Top3 by OOF AUC:', top3)\n",
    "\n",
    "oof_matrix = np.column_stack([oof_preds[name] for name in model_names])\n",
    "test_matrix = np.column_stack([test_preds[name] for name in model_names])\n",
    "if len(top3) > 0:\n",
    "    top3_oof = np.column_stack([oof_preds[name] for name in top3]).mean(axis=1)\n",
    "    top3_test = np.column_stack([test_preds[name] for name in top3]).mean(axis=1)\n",
    "    oof_matrix = np.concatenate([oof_matrix, top3_oof.reshape(-1,1)], axis=1)\n",
    "    test_matrix = np.concatenate([test_matrix, top3_test.reshape(-1,1)], axis=1)\n",
    "    extended_model_names = model_names + ['Top3Avg']\n",
    "else:\n",
    "    extended_model_names = model_names\n",
    "\n",
    "# Meta-learner (probability-calibrated) fitting as default\n",
    "meta_model = LogisticRegression(max_iter=1000)\n",
    "calibrate_meta = True\n",
    "if calibrate_meta:\n",
    "    meta_sample_weight = None\n",
    "    if use_sample_weight:\n",
    "        classes, counts = np.unique(y_train, return_counts=True)\n",
    "        cw_global = {c: sum(counts)/(len(classes)*cnt) for c, cnt in zip(classes, counts)}\n",
    "        if use_pos_weight and positive_label in cw_global:\n",
    "            cw_global[positive_label] = cw_global.get(positive_label, 1.0) * float(pos_weight_multiplier)\n",
    "        meta_sample_weight = pd.Series(y_train).map(cw_global).values\n",
    "    if meta_sample_weight is not None:\n",
    "        meta_model.fit(oof_matrix, y_train, sample_weight=meta_sample_weight)\n",
    "    else:\n",
    "        meta_model.fit(oof_matrix, y_train)\n",
    "    meta_proba = meta_model.predict_proba(oof_matrix)[:,1]\n",
    "    meta_auc = roc_auc_score(y_train, meta_proba)\n",
    "    print('Meta OOF AUC:', meta_auc)\n",
    "    final_test_pred = meta_model.predict_proba(test_matrix)[:,1]\n",
    "\n",
    "    # calibration / shrink / clip\n",
    "    prior = y_train.mean()\n",
    "    prob_calibration_method = 'temperature'\n",
    "    do_shrinkage = False\n",
    "    shrinkage_alpha = 0.05\n",
    "    prob_clip = (1e-6, 1-1e-6)\n",
    "\n",
    "    if prob_calibration_method == 'isotonic':\n",
    "        iso = IsotonicRegression(out_of_bounds='clip')\n",
    "        iso.fit(meta_proba, y_train)\n",
    "        final_test_pred = iso.predict(final_test_pred)\n",
    "    elif prob_calibration_method == 'sigmoid':\n",
    "        platt = LogisticRegression()\n",
    "        platt.fit(meta_proba.reshape(-1,1), y_train)\n",
    "        final_test_pred = platt.predict_proba(final_test_pred.reshape(-1,1))[:,1]\n",
    "    elif prob_calibration_method == 'temperature':\n",
    "        def _apply_temperature(p, T):\n",
    "            eps = 1e-12\n",
    "            p = np.clip(p, eps, 1-eps)\n",
    "            logit = np.log(p / (1-p))\n",
    "            scaled = 1/(1 + np.exp(-logit / T))\n",
    "            return scaled\n",
    "        T_cands = np.linspace(0.5, 2.0, 16)\n",
    "        bestT, bestS = 1.0, -1\n",
    "        stacked_oof = oof_matrix.dot(np.ones(oof_matrix.shape[1]) / oof_matrix.shape[1]) if False else meta_proba\n",
    "        for T in T_cands:\n",
    "            pred_t = _apply_temperature(stacked_oof, T)\n",
    "            try:\n",
    "                a = roc_auc_score(y_train, pred_t)\n",
    "                b = brier_score_loss(y_train, pred_t)\n",
    "                nb = 1 - (b / 0.25)\n",
    "                score = 0.7 * a + 0.3 * nb\n",
    "            except Exception:\n",
    "                score = -1\n",
    "            if score > bestS:\n",
    "                bestS = score\n",
    "                bestT = T\n",
    "        print('Best T:', bestT, 'score:', bestS)\n",
    "        final_test_pred = _apply_temperature(final_test_pred, bestT)\n",
    "\n",
    "    if logit_shift != 0.0:\n",
    "        def _apply_logit_shift(p, s):\n",
    "            eps = 1e-12\n",
    "            p = np.clip(p, eps, 1-eps)\n",
    "            logit = np.log(p / (1-p))\n",
    "            shifted = 1/(1 + np.exp(-(logit + s)))\n",
    "            return shifted\n",
    "        final_test_pred = _apply_logit_shift(final_test_pred, logit_shift)\n",
    "\n",
    "    if do_shrinkage:\n",
    "        final_test_pred = final_test_pred * (1 - shrinkage_alpha) + prior * shrinkage_alpha\n",
    "    final_test_pred = np.clip(final_test_pred, prob_clip[0], prob_clip[1])\n",
    "\n",
    "else:\n",
    "    # 如果不使用 meta learner，可使用随机搜索+精化的权重（此处略）\n",
    "    final_test_pred = np.mean(test_matrix, axis=1)\n",
    "\n",
    "# Save outputs\n",
    "out_df = pd.DataFrame({id_col: test_df[id_col], 'prob': final_test_pred})\n",
    "if isout:\n",
    "    out_df.to_csv(output + output_filename, index=False)\n",
    "    print('Saved submission to', output + output_filename)\n",
    "\n",
    "# Save stacking info\n",
    "joblib = __import__('joblib')\n",
    "joblib.dump({'model_names': extended_model_names, 'oof_preds': oof_preds}, output + 'stacking_weights_oof.pkl')\n",
    "stacking_json = {'timestamp': datetime.datetime.now().isoformat(), 'model_names': extended_model_names, 'oof_preds': {n: oof_preds[n].tolist() for n in model_names}}\n",
    "open(output + 'stacking_weights_oof.json', 'w', encoding='utf-8').write(json.dumps(stacking_json, ensure_ascii=False, indent=2))\n",
    "print('Saved stacking info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ade2e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final pred describe:\n",
      "count    2000.000000\n",
      "mean        0.486562\n",
      "std         0.117680\n",
      "min         0.300409\n",
      "25%         0.384582\n",
      "50%         0.470976\n",
      "75%         0.585491\n",
      "max         0.723273\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Simple diagnostics: print distribution and basic stats\n",
    "try:\n",
    "    print('Final pred describe:')\n",
    "    print(pd.Series(final_test_pred).describe())\n",
    "except Exception as e:\n",
    "    print('Diagnostics skipped (no final_test_pred):', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5efa82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-competition-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
