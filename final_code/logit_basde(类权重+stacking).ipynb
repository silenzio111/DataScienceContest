{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c571abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking 示例（处理类别不平衡后，meta 使用 LogisticRegression C=0.5）\n",
    "# 基准：两个 Logistic(C=1)、一个 CatBoost、一个 LightGBM，最终 meta: Logistic(C=0.5)\n",
    "# 说明：在 fit 时使用 SMOTE 做重采样（目标占比约 20-30%），并使用 5 折 CV 评估 AUC。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ce65600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline as SkPipeline\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c14cb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (500, 24)\n",
      "test shape: (2000, 23)\n",
      "target\n",
      "0    0.98\n",
      "1    0.02\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 路径（根据你的仓库结构调整）\n",
    "ROOT = r'd:\\Competition\\数科统模'\n",
    "TRAIN_PATH = os.path.join(ROOT, 'data', 'data(processed)', 'train.csv')\n",
    "TEST_PATH = os.path.join(ROOT, 'data', 'data(processed)', 'test.csv')\n",
    "\n",
    "# 读取数据（若文件路径不同，请调整）\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print('train shape:', train.shape)\n",
    "print('test shape:', test.shape)\n",
    "\n",
    "# 假设目标列名为 'target'，若不同请修改下面的变量名\n",
    "TARGET = 'target'\n",
    "if TARGET not in train.columns:\n",
    "    raise ValueError(f\"目标列 '{TARGET}' 未在 train 文件中找到，请检查 train.csv 列名。\")\n",
    "\n",
    "# 简单检查目标分布\n",
    "print(train[TARGET].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e84706c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 numeric cols and 0 categorical cols to process\n",
      "features after FE: 48\n",
      "features after FE: 48\n"
     ]
    }
   ],
   "source": [
    "# 基于 baseline 的特征扩展（行统计、交叉、类别计数、KFold 目标编码），然后准备 X/y\n",
    "drop_cols = [c for c in ['id', 'ID', 'index'] if c in train.columns]\n",
    "# 先复制一份以免改动原始 DataFrame（便于调试）\n",
    "train_fe = train.copy()\n",
    "test_fe = test.copy()\n",
    "\n",
    "# 自动识别数值与类别列（排除 id 与 target）\n",
    "feature_cols = [c for c in train.columns if c not in drop_cols + [TARGET]]\n",
    "num_cols = train[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in feature_cols if c not in num_cols]\n",
    "print(f'Found {len(num_cols)} numeric cols and {len(cat_cols)} categorical cols to process')\n",
    "\n",
    "# 1) 行统计（针对数值列）\n",
    "for func_name, func in [('row_sum', np.nansum), ('row_mean', np.nanmean), ('row_std', np.nanstd), ('row_min', np.nanmin), ('row_max', np.nanmax)]:\n",
    "    if len(num_cols) > 0:\n",
    "        train_fe[func_name] = train[num_cols].apply(lambda r: func(r), axis=1)\n",
    "        test_fe[func_name] = test[num_cols].apply(lambda r: func(r), axis=1)\n",
    "    else:\n",
    "        train_fe[func_name] = 0\n",
    "        test_fe[func_name] = 0\n",
    "# 非零计数\n",
    "if len(num_cols) > 0:\n",
    "    train_fe['row_nnz'] = (train[num_cols] != 0).sum(axis=1)\n",
    "    test_fe['row_nnz'] = (test[num_cols] != 0).sum(axis=1)\n",
    "else:\n",
    "    train_fe['row_nnz'] = 0\n",
    "    test_fe['row_nnz'] = 0\n",
    "\n",
    "# 2) 数值交互（乘积与比率）：限制数量以免特征爆炸\n",
    "max_interactions = 20\n",
    "interactions = []\n",
    "for i in range(min(len(num_cols), 50)):\n",
    "    for j in range(i+1, min(len(num_cols), 50)):\n",
    "        interactions.append((num_cols[i], num_cols[j]))\n",
    "        if len(interactions) >= max_interactions:\n",
    "            break\n",
    "    if len(interactions) >= max_interactions:\n",
    "        break\n",
    "\n",
    "# 定义 eps（防止除零），并生成交互特征\n",
    "eps = 1e-6\n",
    "created_interactions = []\n",
    "for a, b in interactions:\n",
    "    new_name_mul = f'{a}_mul_{b}'\n",
    "    new_name_div = f'{a}_div_{b}'\n",
    "    # 乘积\n",
    "    train_fe[new_name_mul] = train_fe[a].fillna(0) * train_fe[b].fillna(0)\n",
    "    test_fe[new_name_mul] = test_fe[a].fillna(0) * test_fe[b].fillna(0)\n",
    "    # 比率（加小常数防除零）\n",
    "    train_fe[new_name_div] = train_fe[a].fillna(0) / (train_fe[b].fillna(0) + eps)\n",
    "    test_fe[new_name_div] = test_fe[a].fillna(0) / (test_fe[b].fillna(0) + eps)\n",
    "    created_interactions.append((a, b, new_name_mul, new_name_div))\n",
    "\n",
    "# 过滤：若交互特征与目标的相关度（绝对值）低于 0.5（Spearman），则删除该交互特征\n",
    "kept_pairs = set()\n",
    "for a, b, mul_name, div_name in created_interactions:\n",
    "    # 计算与目标的 Spearman 等级相关系数（对缺失值进行剔除）\n",
    "    try:\n",
    "        corr_mul = train_fe[mul_name].corr(train_fe[TARGET], method='spearman')\n",
    "    except Exception:\n",
    "        corr_mul = 0\n",
    "    try:\n",
    "        corr_div = train_fe[div_name].corr(train_fe[TARGET], method='spearman')\n",
    "    except Exception:\n",
    "        corr_div = 0\n",
    "    # 只保留任一相关系数绝对值 >= 0.5 的交互（至少一个与目标关联较强）\n",
    "    if abs(corr_mul) >= 0.5 or abs(corr_div) >= 0.5:\n",
    "        kept_pairs.add((a, b))\n",
    "    else:\n",
    "        # 删除低相关交互\n",
    "        for nm in (mul_name, div_name):\n",
    "            if nm in train_fe.columns:\n",
    "                train_fe.drop(columns=[nm], inplace=True)\n",
    "            if nm in test_fe.columns:\n",
    "                test_fe.drop(columns=[nm], inplace=True)\n",
    "\n",
    "# 记录已经用于交互的特征（无序对）\n",
    "used_pairs = set(tuple(sorted((a, b))) for a, b in kept_pairs)\n",
    "\n",
    "# 重新构造当前数值列列表（包含新交互留下的列）\n",
    "current_num_cols = train_fe.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# 计算特征间相关矩阵（只看数值特征，使用 Spearman），寻找相关度 > 0.8 的对，用于生成新的交互（但不要和已交互过的特征交互）\n",
    "corr_matrix = train_fe[current_num_cols].corr(method='spearman').abs()\n",
    "high_corr_pairs = []\n",
    "cols = corr_matrix.columns.tolist()\n",
    "for i in range(len(cols)):\n",
    "    for j in range(i+1, len(cols)):\n",
    "        f1, f2 = cols[i], cols[j]\n",
    "        # 跳过自相关与非数值、本身是交互产生的列（可选）\n",
    "        if f1 == f2:\n",
    "            continue\n",
    "        # 如果两者任一不是原始数值列也可以考虑，但避免与自己交互过的特征重复\n",
    "        pair_sorted = tuple(sorted((f1, f2)))\n",
    "        # 检查是否原始特征对（只考虑原始 feature 列名，不含 '_mul_' 或 '_div_'）\n",
    "        # 我们只避免重复原始对，如果 pair_sorted 包含 interaction 标记也允许\n",
    "        if corr_matrix.loc[f1, f2] > 0.8:\n",
    "            # ensure not interacting a feature with itself and not already used as original pair\n",
    "            orig_pair = tuple(sorted((f1, f2)))\n",
    "            # If orig_pair corresponds to original features (without _mul_/_div_), skip if used\n",
    "            base_f1 = f1.split('_mul_')[0].split('_div_')[0]\n",
    "            base_f2 = f2.split('_mul_')[0].split('_div_')[0]\n",
    "            base_pair = tuple(sorted((base_f1, base_f2)))\n",
    "            if base_pair in used_pairs:\n",
    "                continue\n",
    "            high_corr_pairs.append((f1, f2))\n",
    "\n",
    "# 生成基于高相关性的额外交互（限额以防爆炸），并更新 used_pairs\n",
    "max_additional = 20\n",
    "added = 0\n",
    "for f1, f2 in high_corr_pairs:\n",
    "    if added >= max_additional:\n",
    "        break\n",
    "    # 避免与自己已有交互重复（按 base names）\n",
    "    base_f1 = f1.split('_mul_')[0].split('_div_')[0]\n",
    "    base_f2 = f2.split('_mul_')[0].split('_div_')[0]\n",
    "    base_pair = tuple(sorted((base_f1, base_f2)))\n",
    "    if base_pair in used_pairs or base_f1 == base_f2:\n",
    "        continue\n",
    "    nm_mul = f'{base_f1}_mul_{base_f2}'\n",
    "    nm_div = f'{base_f1}_div_{base_f2}'\n",
    "    train_fe[nm_mul] = train_fe[base_f1].fillna(0) * train_fe[base_f2].fillna(0)\n",
    "    test_fe[nm_mul] = test_fe[base_f1].fillna(0) * test_fe[base_f2].fillna(0)\n",
    "    train_fe[nm_div] = train_fe[base_f1].fillna(0) / (train_fe[base_f2].fillna(0) + eps)\n",
    "    test_fe[nm_div] = test_fe[base_f1].fillna(0) / (test_fe[base_f2].fillna(0) + eps)\n",
    "    used_pairs.add(base_pair)\n",
    "    added += 1\n",
    "\n",
    "# 最终的特征矩阵（按训练的列顺序对齐测试集）\n",
    "X = train_fe.drop(columns=drop_cols + [TARGET], errors='ignore')\n",
    "y = train_fe[TARGET].values\n",
    "X_test = test_fe.drop(columns=drop_cols, errors='ignore')\n",
    "# 确保测试集列与训练集一致（缺失列补 0）\n",
    "for c in X.columns:\n",
    "    if c not in X_test.columns:\n",
    "        X_test[c] = 0\n",
    "X_test = X_test[X.columns.tolist()]\n",
    "\n",
    "print('features after FE:', X.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_weight (neg/pos): 49.0000\n"
     ]
    }
   ],
   "source": [
    "# 定义基学习器（使用类权重，不使用 SMOTE）\n",
    "# 计算正负样本比例\n",
    "pos = int((y == 1).sum())\n",
    "neg = int((y == 0).sum())\n",
    "if pos == 0:\n",
    "    raise ValueError(\"正类样本为 0，无法训练二分类模型，请检查数据/目标列。\")\n",
    "pos_weight = neg / max(pos, 1)\n",
    "print(f\"pos_weight (neg/pos): {pos_weight:.4f}\")\n",
    "\n",
    "# 基学习器\n",
    "lr1 = LogisticRegression(solver='liblinear', penalty='l2', C=1.0, max_iter=10000,\n",
    "                         random_state=42, class_weight='balanced')\n",
    "lr2 = LogisticRegression(solver='liblinear', penalty='l2', C=0.2, max_iter=10000,\n",
    "                         random_state=0, class_weight='balanced')\n",
    "cat = CatBoostClassifier(verbose=0, random_state=42, class_weights=[1.0, float(pos_weight)])\n",
    "lgb = LGBMClassifier(random_state=42, n_jobs=-1, scale_pos_weight=pos_weight)\n",
    "\n",
    "# Stacking 元学习器（meta）\n",
    "meta = LogisticRegression(solver='liblinear', penalty='l2', C=0.5, max_iter=1000,\n",
    "                          random_state=42, class_weight='balanced')\n",
    "\n",
    "estimators = [\n",
    "    ('lr1', lr1),\n",
    "    ('lr2', lr2),\n",
    "    ('cat', cat),\n",
    "    ('lgb', lgb)\n",
    "]\n",
    "\n",
    "stack = StackingClassifier(estimators=estimators, final_estimator=meta, cv=5, n_jobs=-1, passthrough=False)\n",
    "\n",
    "# 使用 sklearn Pipeline，把填充、标准化（主要对 LR 有效）和 stacking 串起来\n",
    "pipeline = SkPipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('stack', stack)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC scores: [0.32653061 0.47959184 0.45918367 0.51020408 0.03571429]\n",
      "Mean AUC: 0.362245 (+/- 0.1749)\n"
     ]
    }
   ],
   "source": [
    "# 交叉验证评估（AUC）\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(pipeline, X, y, cv=cv, scoring='roc_auc', n_jobs=1)\n",
    "print('CV AUC scores:', scores)\n",
    "print('Mean AUC: %.6f (+/- %.4f)' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac1c42af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission to d:\\Competition\\数科统模\\submit\\stacking_v2_logit_meta_logitC05_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# 在全部训练数据上训练并对测试集做预测（概率）\n",
    "pipeline.fit(X, y)\n",
    "if hasattr(pipeline, 'predict_proba') or hasattr(pipeline.named_steps['stack'], 'predict_proba'):\n",
    "    proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "else:\n",
    "    # fallback: decision_function -> scale to [0,1] via sigmoid\n",
    "    from scipy.special import expit\n",
    "    dec = pipeline.decision_function(X_test)\n",
    "    proba = expit(dec)\n",
    "\n",
    "# 构建提交文件（假设 test 有 'id' 列或使用索引）\n",
    "id_col = None\n",
    "for c in ['id', 'ID', 'index']:\n",
    "    if c in test.columns:\n",
    "        id_col = c\n",
    "        break\n",
    "if id_col is None:\n",
    "    submission = pd.DataFrame({'id': np.arange(len(proba)), 'target': proba})\n",
    "else:\n",
    "    submission = pd.DataFrame({id_col: test[id_col].values, 'target': proba})\n",
    "\n",
    "out_dir = os.path.join(ROOT, 'submit')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_path = os.path.join(out_dir, 'stacking_v2_logit_meta_logitC05_submission.csv')\n",
    "submission.to_csv(out_path, index=False)\n",
    "print('Saved submission to', out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a330563",
   "metadata": {},
   "source": [
    "## 改进版方案：去除 SMOTE，使用类权重 + 为不同基学习器定制预处理 + 增加 XGBoost + passthrough\n",
    "\n",
    "动机与变更：\n",
    "- 不再对所有模型统一做标准化/SMOTE，转而：\n",
    "  - 树模型（LightGBM/CatBoost/XGBoost）使用类权重（scale_pos_weight/class_weights），仅使用缺失值填充；\n",
    "  - 线性模型（Logistic）保留标准化并使用 class_weight='balanced'；\n",
    "- Stacking 使用 passthrough=True，让 meta 能看到原始（填充后）的特征 + 各基模型 OOF 概率；\n",
    "- 新增 XGBoost 提升树模型多样性；\n",
    "- 验证改用 RepeatedStratifiedKFold，降低单次分割的方差；\n",
    "- 期望更稳健，避免 SMOTE 在高维/树模型上潜在的过拟合与噪声放大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f86ccdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_weight (neg/pos): 49.0000\n",
      "Repeated CV AUC: [0.530612 0.311224 0.52551  0.591837 0.566327 0.836735 0.362245 0.086735\n",
      " 0.586735 0.515306]\n",
      "Mean AUC: 0.491327 (+/- 0.1898)\n",
      "Saved submission to d:\\Competition\\数科统模\\submit\\stacking_v2_classweight_passthrough_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# 改进版：类权重 + 定制预处理 + XGBoost + passthrough\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline as SkPipeline\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 计算正负样本比例（供各模型类权重使用）\n",
    "pos = (y == 1).sum()\n",
    "neg = (y == 0).sum()\n",
    "if pos == 0:\n",
    "    raise ValueError(\"正类样本为 0，无法训练二分类模型，请检查数据/目标列。\")\n",
    "pos_weight = neg / max(pos, 1)\n",
    "print(f\"pos_weight (neg/pos): {pos_weight:.4f}\")\n",
    "\n",
    "# 基学习器：\n",
    "# 线性模型使用标准化 + class_weight；树模型仅填充缺失并通过内部不平衡参数处理\n",
    "lr1 = SkPipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(solver='liblinear', penalty='l2', C=1.0,\n",
    "                               max_iter=10000, class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "lr2 = SkPipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(solver='liblinear', penalty='l2', C=0.2,\n",
    "                               max_iter=10000, class_weight='balanced', random_state=0))\n",
    "])\n",
    "\n",
    "lgb = LGBMClassifier(\n",
    "    objective='binary', metric='auc',\n",
    "    n_estimators=800, learning_rate=0.05,\n",
    "    num_leaves=31, subsample=0.8, colsample_bytree=0.8,\n",
    "    reg_alpha=0.1, reg_lambda=0.1,\n",
    "    n_jobs=-1, random_state=42,\n",
    "    scale_pos_weight=pos_weight\n",
    ")\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=800, learning_rate=0.05, max_depth=5,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    reg_alpha=0.1, reg_lambda=1.0,\n",
    "    objective='binary:logistic', eval_metric='auc',\n",
    "    tree_method='hist', n_jobs=-1, random_state=42,\n",
    "    scale_pos_weight=pos_weight\n",
    ")\n",
    "\n",
    "cat = CatBoostClassifier(\n",
    "    iterations=800, learning_rate=0.05, depth=6,\n",
    "    loss_function='Logloss', eval_metric='AUC',\n",
    "    random_seed=42, verbose=0,\n",
    "    class_weights=[1.0, float(pos_weight)]\n",
    ")\n",
    "\n",
    "estimators = [\n",
    "    ('lr1', lr1),\n",
    "    ('lr2', lr2),\n",
    "    ('lgb', lgb),\n",
    "    ('xgb', xgb),\n",
    "    ('cat', cat)\n",
    "]\n",
    "\n",
    "# meta 模型也做标准化并使用类权重（输入为各基模型预测 + 原始特征 passthrough）\n",
    "meta = SkPipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(solver='liblinear', penalty='l2', C=0.5,\n",
    "                               max_iter=2000, class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "# 顶层：先做缺失值填充，确保 passthrough 部分没有 NaN\n",
    "model = SkPipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('stack', StackingClassifier(\n",
    "        estimators=estimators,\n",
    "        final_estimator=meta,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        passthrough=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 验证：重复分层 5x2，降低单次切分的方差\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=42)\n",
    "scores = cross_val_score(model, X, y, cv=rskf, scoring='roc_auc', n_jobs=1)\n",
    "print('Repeated CV AUC:', np.round(scores, 6))\n",
    "print('Mean AUC: %.6f (+/- %.4f)' % (scores.mean(), scores.std()))\n",
    "\n",
    "# 训练与预测并导出提交\n",
    "model.fit(X, y)\n",
    "proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 安全构建提交：自动识别 id 列，无则用顺序 id\n",
    "submission_v2 = pd.DataFrame({'id': np.arange(len(proba)), 'target': proba})\n",
    "\n",
    "\n",
    "out_dir = os.path.join(ROOT, 'submit')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_path_v2 = os.path.join(out_dir, 'stacking_v2_classweight_passthrough_submission.csv')\n",
    "submission_v2.to_csv(out_path_v2, index=False)\n",
    "print('Saved submission to', out_path_v2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-competition-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
