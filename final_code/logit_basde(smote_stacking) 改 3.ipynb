{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c571abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking 示例（处理类别不平衡后，meta 使用 LogisticRegression C=0.5）\n",
    "# 基准：两个 Logistic(C=1)、一个 CatBoost、一个 LightGBM，最终 meta: Logistic(C=0.5)\n",
    "# 说明：在 fit 时使用 SMOTE 做重采样（目标占比约 20-30%），并使用 5 折 CV 评估 AUPRC (average precision)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ce65600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\n",
    "from sklearn.metrics import average_precision_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c14cb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (500, 24)\n",
      "test shape: (2000, 23)\n",
      "target\n",
      "0    0.98\n",
      "1    0.02\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 路径（根据你的仓库结构调整）\n",
    "ROOT = r'd:\\Competition\\数科统模'\n",
    "TRAIN_PATH = os.path.join(ROOT, 'data', 'data(processed)', 'train.csv')\n",
    "TEST_PATH = os.path.join(ROOT, 'data', 'data(processed)', 'test.csv')\n",
    "\n",
    "# 读取数据（若文件路径不同，请调整）\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print('train shape:', train.shape)\n",
    "print('test shape:', test.shape)\n",
    "\n",
    "# 假设目标列名为 'target'，若不同请修改下面的变量名\n",
    "TARGET = 'target'\n",
    "\n",
    "# 简单检查目标分布\n",
    "print(train[TARGET].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e84706c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 numeric cols and 0 categorical cols to process\n",
      "features after FE: 48\n",
      "features after FE: 48\n"
     ]
    }
   ],
   "source": [
    "# 基于 baseline 的特征扩展（行统计、交叉、类别计数、KFold 目标编码），然后准备 X/y\n",
    "drop_cols = [c for c in ['id', 'ID', 'index'] if c in train.columns]\n",
    "# 先复制一份以免改动原始 DataFrame（便于调试）\n",
    "train_fe = train.copy()\n",
    "test_fe = test.copy()\n",
    "\n",
    "# 自动识别数值与类别列（排除 id 与 target）\n",
    "feature_cols = [c for c in train.columns if c not in drop_cols + [TARGET]]\n",
    "num_cols = train[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in feature_cols if c not in num_cols]\n",
    "print(f'Found {len(num_cols)} numeric cols and {len(cat_cols)} categorical cols to process')\n",
    "\n",
    "# 1) 行统计（针对数值列）\n",
    "for func_name, func in [('row_sum', np.nansum), ('row_mean', np.nanmean), ('row_std', np.nanstd), ('row_min', np.nanmin), ('row_max', np.nanmax)]:\n",
    "    if len(num_cols) > 0:\n",
    "        train_fe[func_name] = train[num_cols].apply(lambda r: func(r), axis=1)\n",
    "        test_fe[func_name] = test[num_cols].apply(lambda r: func(r), axis=1)\n",
    "    else:\n",
    "        train_fe[func_name] = 0\n",
    "        test_fe[func_name] = 0\n",
    "# 非零计数\n",
    "if len(num_cols) > 0:\n",
    "    train_fe['row_nnz'] = (train[num_cols] != 0).sum(axis=1)\n",
    "    test_fe['row_nnz'] = (test[num_cols] != 0).sum(axis=1)\n",
    "else:\n",
    "    train_fe['row_nnz'] = 0\n",
    "    test_fe['row_nnz'] = 0\n",
    "\n",
    "# 2) 数值交互（乘积与比率）：限制数量以免特征爆炸\n",
    "max_interactions = 25\n",
    "interactions = []\n",
    "for i in range(min(len(num_cols), 50)):\n",
    "    for j in range(i+1, min(len(num_cols), 50)):\n",
    "        interactions.append((num_cols[i], num_cols[j]))\n",
    "        if len(interactions) >= max_interactions:\n",
    "            break\n",
    "    if len(interactions) >= max_interactions:\n",
    "        break\n",
    "\n",
    "# 定义 eps（防止除零），并生成交互特征\n",
    "eps = 1e-6\n",
    "created_interactions = []\n",
    "for a, b in interactions:\n",
    "    new_name_mul = f'{a}_mul_{b}'\n",
    "    new_name_div = f'{a}_div_{b}'\n",
    "    # 乘积\n",
    "    train_fe[new_name_mul] = train_fe[a].fillna(0) * train_fe[b].fillna(0)\n",
    "    test_fe[new_name_mul] = test_fe[a].fillna(0) * test_fe[b].fillna(0)\n",
    "    # 比率（加小常数防除零）\n",
    "    train_fe[new_name_div] = train_fe[a].fillna(0) / (train_fe[b].fillna(0) + eps)\n",
    "    test_fe[new_name_div] = test_fe[a].fillna(0) / (test_fe[b].fillna(0) + eps)\n",
    "    created_interactions.append((a, b, new_name_mul, new_name_div))\n",
    "\n",
    "# 过滤：若交互特征与目标的相关度（绝对值）低于 0.5（Spearman），则删除该交互特征\n",
    "kept_pairs = set()\n",
    "for a, b, mul_name, div_name in created_interactions:\n",
    "    # 计算与目标的 Spearman 等级相关系数（对缺失值进行剔除）\n",
    "    try:\n",
    "        corr_mul = train_fe[mul_name].corr(train_fe[TARGET], method='spearman')\n",
    "    except Exception:\n",
    "        corr_mul = 0\n",
    "    try:\n",
    "        corr_div = train_fe[div_name].corr(train_fe[TARGET], method='spearman')\n",
    "    except Exception:\n",
    "        corr_div = 0\n",
    "    # 只保留任一相关系数绝对值 >= 0.5 的交互（至少一个与目标关联较强）\n",
    "    if abs(corr_mul) >= 0.5 or abs(corr_div) >= 0.5:\n",
    "        kept_pairs.add((a, b))\n",
    "    else:\n",
    "        # 删除低相关交互\n",
    "        for nm in (mul_name, div_name):\n",
    "            if nm in train_fe.columns:\n",
    "                train_fe.drop(columns=[nm], inplace=True)\n",
    "            if nm in test_fe.columns:\n",
    "                test_fe.drop(columns=[nm], inplace=True)\n",
    "\n",
    "# 记录已经用于交互的特征（无序对）\n",
    "used_pairs = set(tuple(sorted((a, b))) for a, b in kept_pairs)\n",
    "\n",
    "# 重新构造当前数值列列表（包含新交互留下的列）\n",
    "current_num_cols = train_fe.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# 计算特征间相关矩阵（只看数值特征，使用 Spearman），寻找相关度 > 0.8 的对，用于生成新的交互（但不要和已交互过的特征交互）\n",
    "corr_matrix = train_fe[current_num_cols].corr(method='spearman').abs()\n",
    "high_corr_pairs = []\n",
    "cols = corr_matrix.columns.tolist()\n",
    "for i in range(len(cols)):\n",
    "    for j in range(i+1, len(cols)):\n",
    "        f1, f2 = cols[i], cols[j]\n",
    "        # 跳过自相关与非数值、本身是交互产生的列（可选）\n",
    "        if f1 == f2:\n",
    "            continue\n",
    "        # 如果两者任一不是原始数值列也可以考虑，但避免与自己交互过的特征重复\n",
    "        pair_sorted = tuple(sorted((f1, f2)))\n",
    "        # 检查是否原始特征对（只考虑原始 feature 列名，不含 '_mul_' 或 '_div_'）\n",
    "        # 我们只避免重复原始对，如果 pair_sorted 包含 interaction 标记也允许\n",
    "        if corr_matrix.loc[f1, f2] > 0.8:\n",
    "            # ensure not interacting a feature with itself and not already used as original pair\n",
    "            orig_pair = tuple(sorted((f1, f2)))\n",
    "            # If orig_pair corresponds to original features (without _mul_/_div_), skip if used\n",
    "            base_f1 = f1.split('_mul_')[0].split('_div_')[0]\n",
    "            base_f2 = f2.split('_mul_')[0].split('_div_')[0]\n",
    "            base_pair = tuple(sorted((base_f1, base_f2)))\n",
    "            if base_pair in used_pairs:\n",
    "                continue\n",
    "            high_corr_pairs.append((f1, f2))\n",
    "\n",
    "# 生成基于高相关性的额外交互（限额以防爆炸），并更新 used_pairs\n",
    "max_additional = 20\n",
    "added = 0\n",
    "for f1, f2 in high_corr_pairs:\n",
    "    if added >= max_additional:\n",
    "        break\n",
    "    # 避免与自己已有交互重复（按 base names）\n",
    "    base_f1 = f1.split('_mul_')[0].split('_div_')[0]\n",
    "    base_f2 = f2.split('_mul_')[0].split('_div_')[0]\n",
    "    base_pair = tuple(sorted((base_f1, base_f2)))\n",
    "    if base_pair in used_pairs or base_f1 == base_f2:\n",
    "        continue\n",
    "    nm_mul = f'{base_f1}_mul_{base_f2}'\n",
    "    nm_div = f'{base_f1}_div_{base_f2}'\n",
    "    train_fe[nm_mul] = train_fe[base_f1].fillna(0) * train_fe[base_f2].fillna(0)\n",
    "    test_fe[nm_mul] = test_fe[base_f1].fillna(0) * test_fe[base_f2].fillna(0)\n",
    "    train_fe[nm_div] = train_fe[base_f1].fillna(0) / (train_fe[base_f2].fillna(0) + eps)\n",
    "    test_fe[nm_div] = test_fe[base_f1].fillna(0) / (test_fe[base_f2].fillna(0) + eps)\n",
    "    used_pairs.add(base_pair)\n",
    "    added += 1\n",
    "\n",
    "# 最终的特征矩阵（按训练的列顺序对齐测试集）\n",
    "X = train_fe.drop(columns=drop_cols + [TARGET], errors='ignore')\n",
    "y = train_fe[TARGET].values\n",
    "X_test = test_fe.drop(columns=drop_cols, errors='ignore')\n",
    "# 确保测试集列与训练集一致（缺失列补 0）\n",
    "for c in X.columns:\n",
    "    if c not in X_test.columns:\n",
    "        X_test[c] = 0\n",
    "X_test = X_test[X.columns.tolist()]\n",
    "\n",
    "print('features after FE:', X.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0d0c09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两阶段超参搜索工具与辅助方法（含树模型早停微调）\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, StratifiedKFold, ParameterGrid\n",
    "from sklearn.base import clone\n",
    "from scipy.stats import loguniform, randint, uniform\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb  # for train/DMatrix\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# 邻域生成：围绕浮点/整数参数做小范围微调\n",
    "\n",
    "def _neighbors_float(v, low, high):\n",
    "    vals = [v * 0.7, v * 0.85, v, v * 1.15, v * 1.3]\n",
    "    vals = [min(max(float(x), low), high) for x in vals]\n",
    "    vals = sorted({round(x, 6) for x in vals})\n",
    "    return vals\n",
    "\n",
    "\n",
    "\n",
    "def _neighbors_int(v, low, high):\n",
    "    vals = [int(round(v - 1)), int(round(v)), int(round(v + 1))]\n",
    "    vals = [min(max(int(x), low), high) for x in vals]\n",
    "    vals = sorted(set(vals))\n",
    "    return vals\n",
    "\n",
    "\n",
    "# Stage-1 搜索空间（按模型）\n",
    "\n",
    "def _get_stage1_spaces(model_name):\n",
    "    spaces = {}\n",
    "    use_scaler = False\n",
    "    if model_name in (\"lr1\", \"lr2\", \"meta\"):\n",
    "        use_scaler = True\n",
    "        spaces = {\n",
    "            'model__C': loguniform(1e-3, 3.0),  # 约 0.001~3\n",
    "        }\n",
    "    elif model_name == 'lgb':\n",
    "        spaces = {\n",
    "            'model__n_estimators': randint(150, 600),\n",
    "            'model__learning_rate': loguniform(0.01, 0.3),\n",
    "            'model__num_leaves': randint(16, 64),\n",
    "            'model__max_depth': randint(3, 12),\n",
    "            'model__subsample': uniform(0.6, 0.4),          # 0.6~1.0\n",
    "            'model__colsample_bytree': uniform(0.6, 0.4),   # 0.6~1.0\n",
    "            'model__reg_lambda': loguniform(1e-3, 10.0),\n",
    "            'model__reg_alpha': loguniform(1e-3, 10.0),\n",
    "        }\n",
    "    elif model_name == 'cat':\n",
    "        spaces = {\n",
    "            'model__depth': randint(4, 10),\n",
    "            'model__learning_rate': loguniform(0.01, 0.3),\n",
    "            'model__iterations': randint(200, 700),\n",
    "            'model__l2_leaf_reg': loguniform(1.0, 10.0),\n",
    "        }\n",
    "    elif model_name == 'xgb':\n",
    "        spaces = {\n",
    "            'model__n_estimators': randint(200, 700),\n",
    "            'model__max_depth': randint(3, 10),\n",
    "            'model__learning_rate': loguniform(0.01, 0.3),\n",
    "            'model__subsample': uniform(0.6, 0.4),\n",
    "            'model__colsample_bytree': uniform(0.6, 0.4),\n",
    "            'model__min_child_weight': randint(1, 7),\n",
    "            'model__gamma': loguniform(1e-3, 5.0),\n",
    "            'model__reg_lambda': loguniform(1e-3, 10.0),\n",
    "            'model__reg_alpha': loguniform(1e-3, 1.0),\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"未知模型名: {model_name}\")\n",
    "    return spaces, use_scaler\n",
    "\n",
    "\n",
    "# Stage-2：围绕 Stage-1 的前 top_k 结果构建精细网格（小范围）\n",
    "\n",
    "def _stage2_grid_from_best(model_name, best_params):\n",
    "    grid = {}\n",
    "    for k, v in best_params.items():\n",
    "        if not k.startswith('model__'):\n",
    "            continue\n",
    "        pname = k.split('__', 1)[1]\n",
    "        if model_name in (\"lr1\", \"lr2\", \"meta\") and pname == 'C':\n",
    "            grid[k] = _neighbors_float(float(v), 1e-4, 100.0)\n",
    "        elif model_name == 'lgb':\n",
    "            if pname == 'n_estimators':\n",
    "                grid[k] = _neighbors_int(int(v), 100, 1000)\n",
    "            elif pname == 'learning_rate':\n",
    "                grid[k] = _neighbors_float(float(v), 0.005, 0.5)\n",
    "            elif pname == 'num_leaves':\n",
    "                grid[k] = _neighbors_int(int(v), 8, 128)\n",
    "            elif pname == 'max_depth':\n",
    "                grid[k] = _neighbors_int(int(v), 3, 16)\n",
    "            elif pname == 'subsample':\n",
    "                grid[k] = _neighbors_float(float(v), 0.5, 1.0)\n",
    "            elif pname == 'colsample_bytree':\n",
    "                grid[k] = _neighbors_float(float(v), 0.5, 1.0)\n",
    "            elif pname == 'reg_lambda':\n",
    "                grid[k] = _neighbors_float(float(v), 1e-4, 100.0)\n",
    "            elif pname == 'reg_alpha':\n",
    "                grid[k] = _neighbors_float(float(v), 1e-4, 100.0)\n",
    "        elif model_name == 'cat':\n",
    "            if pname == 'depth':\n",
    "                grid[k] = _neighbors_int(int(v), 3, 12)\n",
    "            elif pname == 'learning_rate':\n",
    "                grid[k] = _neighbors_float(float(v), 0.005, 0.5)\n",
    "            elif pname == 'iterations':\n",
    "                grid[k] = _neighbors_int(int(v), 100, 1200)\n",
    "            elif pname == 'l2_leaf_reg':\n",
    "                grid[k] = _neighbors_float(float(v), 1e-2, 100.0)\n",
    "        elif model_name == 'xgb':\n",
    "            if pname == 'n_estimators':\n",
    "                grid[k] = _neighbors_int(int(v), 100, 1200)\n",
    "            elif pname == 'max_depth':\n",
    "                grid[k] = _neighbors_int(int(v), 3, 16)\n",
    "            elif pname == 'learning_rate':\n",
    "                grid[k] = _neighbors_float(float(v), 0.005, 0.5)\n",
    "            elif pname == 'subsample':\n",
    "                grid[k] = _neighbors_float(float(v), 0.5, 1.0)\n",
    "            elif pname == 'colsample_bytree':\n",
    "                grid[k] = _neighbors_float(float(v), 0.5, 1.0)\n",
    "            elif pname == 'min_child_weight':\n",
    "                grid[k] = _neighbors_int(int(v), 1, 10)\n",
    "            elif pname == 'gamma':\n",
    "                grid[k] = _neighbors_float(float(v), 0.0, 10.0)\n",
    "            elif pname == 'reg_lambda':\n",
    "                grid[k] = _neighbors_float(float(v), 1e-4, 100.0)\n",
    "            elif pname == 'reg_alpha':\n",
    "                grid[k] = _neighbors_float(float(v), 1e-4, 10.0)\n",
    "    return grid\n",
    "\n",
    "\n",
    "def _clone_with_updated_params(estimator, **updates):\n",
    "    \"\"\"返回一个新的未拟合实例，参数为原始 estimator 的 get_params 并更新 updates。\"\"\"\n",
    "    params = estimator.get_params(deep=False)\n",
    "    params.update(updates)\n",
    "    return estimator.__class__(**params)\n",
    "\n",
    "\n",
    "def _post_early_stopping_refit(model_name, base_model, X, y, random_state=42, stopping_rounds=50, use_focal=False):\n",
    "    \"\"\"在小的验证切分上做一次早停以确定合适的迭代数，然后返回\"未拟合\"的新实例（带最佳迭代数）。\n",
    "    支持可选的 focal loss（混合参数），但默认关闭以保证稳定性。\"\"\"\n",
    "    # 快速单折验证（取 KFold 第一折作为 val）\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    tr_idx, val_idx = next(iter(cv.split(X, y)))\n",
    "    X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "    X_val, y_val = X.iloc[val_idx], y[val_idx]\n",
    "\n",
    "    # 与搜索阶段一致的预处理：imputer + （对 LR/Meta 加 scaler）+ SMOTE（仅训练集）\n",
    "    use_scaler = model_name in (\"lr1\", \"lr2\", \"meta\")\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_tr_imp = imputer.fit_transform(X_tr)\n",
    "    X_val_imp = imputer.transform(X_val)\n",
    "\n",
    "    if use_scaler:\n",
    "        scaler = StandardScaler()\n",
    "        X_tr_imp = scaler.fit_transform(X_tr_imp)\n",
    "        X_val_imp = scaler.transform(X_val_imp)\n",
    "\n",
    "    # 仅对线性模型在此处做 SMOTE（训练集内），树模型使用类权重/scale_pos_weight\n",
    "    if model_name in (\"lr1\", \"lr2\", \"meta\"):\n",
    "        sm = SMOTE(sampling_strategy=0.25, random_state=random_state)\n",
    "        X_tr_imp, y_tr = sm.fit_resample(X_tr_imp, y_tr)\n",
    "\n",
    "    # 克隆并拟合用于获得 best_iteration\n",
    "    mdl = clone(base_model)\n",
    "\n",
    "    # LightGBM: 使用 lgb.train 实现早停并可选 focal loss\n",
    "    if model_name == 'lgb' and isinstance(mdl, LGBMClassifier):\n",
    "        ne = int(getattr(mdl, 'n_estimators', 400))\n",
    "        params = mdl.get_params(deep=False)\n",
    "        # 设置为较大迭代数让 early_stopping 生效\n",
    "        num_boost_round = max(800, ne)\n",
    "        lgb_train = lgb.Dataset(X_tr_imp, label=y_tr)\n",
    "        lgb_val = lgb.Dataset(X_val_imp, label=y_val)\n",
    "        # focal loss 自定义（简化版，谨慎使用）\n",
    "        def focal_lgb(preds, dtrain, alpha=0.25, gamma=2.0):\n",
    "            y_true = dtrain.get_label()\n",
    "            preds_sig = 1.0 / (1.0 + np.exp(-preds))\n",
    "            grad = (preds_sig - y_true) * ((1 - preds_sig) ** gamma)\n",
    "            hess = preds_sig * (1 - preds_sig) * ((1 - preds_sig) ** gamma)\n",
    "            return grad, hess\n",
    "\n",
    "        # 使用 AUPRC (average precision) 作为早停监控指标\n",
    "        params['metric'] = 'average_precision'\n",
    "        if use_focal:\n",
    "            bst = lgb.train(params, lgb_train, num_boost_round=num_boost_round,\n",
    "                            valid_sets=[lgb_val],\n",
    "                            fobj=focal_lgb,\n",
    "                            early_stopping_rounds=stopping_rounds, verbose_eval=False)\n",
    "        else:\n",
    "            bst = lgb.train(params, lgb_train, num_boost_round=num_boost_round,\n",
    "                            valid_sets=[lgb_val],\n",
    "                            early_stopping_rounds=stopping_rounds, verbose_eval=False)\n",
    "        best_iter = getattr(bst, 'best_iteration', None) or getattr(bst, 'best_ntree_limit', None)\n",
    "        if best_iter and best_iter > 0:\n",
    "            return _clone_with_updated_params(mdl, n_estimators=int(best_iter))\n",
    "        return base_model\n",
    "\n",
    "    # XGBoost: 使用原生 API 做早停并可选 focal loss\n",
    "    if model_name == 'xgb' and isinstance(mdl, XGBClassifier):\n",
    "        params = {}\n",
    "        mp = mdl.get_params(deep=False)\n",
    "        if 'max_depth' in mp: params['max_depth'] = mp['max_depth']\n",
    "        if 'learning_rate' in mp: params['eta'] = mp['learning_rate']\n",
    "        if 'subsample' in mp: params['subsample'] = mp['subsample']\n",
    "        if 'colsample_bytree' in mp: params['colsample_bytree'] = mp['colsample_bytree']\n",
    "        if 'min_child_weight' in mp: params['min_child_weight'] = mp['min_child_weight']\n",
    "        if 'gamma' in mp: params['gamma'] = mp['gamma']\n",
    "        if 'reg_lambda' in mp: params['lambda'] = mp['reg_lambda']\n",
    "        if 'reg_alpha' in mp: params['alpha'] = mp['reg_alpha']\n",
    "        if 'tree_method' in mp: params['tree_method'] = mp['tree_method']\n",
    "        if 'n_jobs' in mp: params['nthread'] = mp['n_jobs']\n",
    "        if 'random_state' in mp: params['seed'] = mp['random_state']\n",
    "        params['objective'] = 'binary:logistic'\n",
    "        params['eval_metric'] = 'aucpr'\n",
    "        params['verbosity'] = 0\n",
    "\n",
    "        num_boost_round = max(800, int(mp.get('n_estimators', 400)))\n",
    "        dtrain = xgb.DMatrix(X_tr_imp, label=y_tr)\n",
    "        dvalid = xgb.DMatrix(X_val_imp, label=y_val)\n",
    "\n",
    "        def focal_xgb(preds, dtrain, alpha=0.25, gamma=2.0):\n",
    "            y_true = dtrain.get_label()\n",
    "            preds_sig = 1.0 / (1.0 + np.exp(-preds))\n",
    "            grad = (preds_sig - y_true) * ((1 - preds_sig) ** gamma)\n",
    "            hess = preds_sig * (1 - preds_sig) * ((1 - preds_sig) ** gamma)\n",
    "            return grad, hess\n",
    "\n",
    "        if use_focal:\n",
    "            booster = xgb.train(params, dtrain, num_boost_round=num_boost_round,\n",
    "                                obj=focal_xgb,\n",
    "                                evals=[(dvalid, 'valid')],\n",
    "                                early_stopping_rounds=stopping_rounds, verbose_eval=False)\n",
    "        else:\n",
    "            booster = xgb.train(params, dtrain, num_boost_round=num_boost_round,\n",
    "                                evals=[(dvalid, 'valid')],\n",
    "                                early_stopping_rounds=stopping_rounds, verbose_eval=False)\n",
    "        best_iter = getattr(booster, 'best_iteration', None)\n",
    "        if best_iter is None:\n",
    "            try:\n",
    "                best_iter = int(booster.best_ntree_limit)\n",
    "            except Exception:\n",
    "                best_iter = None\n",
    "        if best_iter and best_iter > 0:\n",
    "            return _clone_with_updated_params(mdl, n_estimators=int(best_iter))\n",
    "        return base_model\n",
    "\n",
    "    # CatBoost: 保持原有逻辑（但使用数值型 class_weights 列表以兼容 CatBoost）\n",
    "    if model_name == 'cat' and isinstance(mdl, CatBoostClassifier):\n",
    "        it = int(getattr(mdl, 'iterations', 400))\n",
    "        mdl.set_params(iterations=max(800, it), use_best_model=True, od_type='Iter', od_wait=stopping_rounds)\n",
    "        mdl.fit(\n",
    "            X_tr_imp, y_tr,\n",
    "            eval_set=(X_val_imp, y_val),\n",
    "            verbose=False\n",
    "        )\n",
    "        best_iter = getattr(mdl, 'best_iteration_', None)\n",
    "        base_params = mdl.get_params(deep=False)\n",
    "        base_params.pop('use_best_model', None)\n",
    "        base_params.pop('od_type', None)\n",
    "        base_params.pop('od_wait', None)\n",
    "        if best_iter and best_iter > 0:\n",
    "            base_params['iterations'] = int(best_iter)\n",
    "        else:\n",
    "            base_params['iterations'] = it\n",
    "        clean_cat = CatBoostClassifier(**base_params)\n",
    "        return clean_cat\n",
    "\n",
    "    # 线性模型无需早停\n",
    "    return base_model\n",
    "\n",
    "\n",
    "def tune_model_two_stage(model_name, base_model, X, y,\n",
    "                         random_state=42,\n",
    "                         stage1_iter=15,\n",
    "                         top_k=3,\n",
    "                         stage2_max_candidates=24,\n",
    "                         enable_post_es=True,\n",
    "                         use_focal=False):\n",
    "    \"\"\"两阶段调参：\n",
    "    - 将 SMOTE 限制为线性模型（lr1, lr2, meta）内折重采样；\n",
    "    - 对树模型（lgb, xgb, cat）使用类权重或 scale_pos_weight 替代全局重采样；\n",
    "    - 支持可选 focal loss（通过 use_focal=True 打开）。\n",
    "    \"\"\"\n",
    "    spaces, use_scaler = _get_stage1_spaces(model_name)\n",
    "\n",
    "    # 为树模型设置类权重 / scale_pos_weight（在 fit 前 clone 避免污染原始实例）\n",
    "    npos = int(np.sum(y == 1))\n",
    "    nneg = int(len(y) - npos)\n",
    "    scale_pos_weight = float(nneg / max(1, npos))\n",
    "    base_clone = clone(base_model)\n",
    "    if model_name == 'xgb':\n",
    "        try:\n",
    "            base_clone.set_params(scale_pos_weight=scale_pos_weight)\n",
    "        except Exception:\n",
    "            pass\n",
    "    elif model_name == 'lgb':\n",
    "        try:\n",
    "            # LGB 可接受 class_weight 或 scale_pos_weight；这里优先设置 class_weight='balanced'\n",
    "            base_clone.set_params(class_weight='balanced')\n",
    "        except Exception:\n",
    "            try:\n",
    "                base_clone.set_params(scale_pos_weight=scale_pos_weight)\n",
    "            except Exception:\n",
    "                pass\n",
    "    elif model_name == 'cat':\n",
    "        try:\n",
    "            # CatBoost 要求 class_weights 为 list（每个类别的权重），使用 [1.0, scale_pos_weight]\n",
    "            base_clone.set_params(class_weights=[1.0, scale_pos_weight])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    steps = [('imputer', SimpleImputer(strategy='median'))]\n",
    "    if use_scaler:\n",
    "        steps.append(('scaler', StandardScaler()))\n",
    "    # 仅对线性模型在管道内做 SMOTE，以保证重采样发生在每个折的训练集上\n",
    "    if model_name in (\"lr1\", \"lr2\", \"meta\"):\n",
    "        steps.append(('smote', SMOTE(sampling_strategy=0.25, random_state=random_state)))\n",
    "    steps.append(('model', base_clone))\n",
    "    pl = ImbPipeline(steps)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Stage 1: 随机粗搜索\n",
    "    rs = RandomizedSearchCV(\n",
    "        pl,\n",
    "        param_distributions=spaces,\n",
    "        n_iter=stage1_iter,\n",
    "        scoring='average_precision',\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        refit=False,\n",
    "        random_state=random_state,\n",
    "        verbose=0,\n",
    "        error_score='raise'\n",
    "    )\n",
    "    rs.fit(X, y)\n",
    "    res = pd.DataFrame(rs.cv_results_).sort_values('rank_test_score')\n",
    "\n",
    "    # 取前 top_k 个作为精调起点\n",
    "    top = res.head(top_k)\n",
    "\n",
    "    # Stage 2: 小范围精细化搜索（构建有限候选集）\n",
    "    candidates = []\n",
    "    for _, row in top.iterrows():\n",
    "        best_params = row['params']\n",
    "        grid = _stage2_grid_from_best(model_name, best_params)\n",
    "        if not grid:\n",
    "            candidates.append(best_params)\n",
    "        else:\n",
    "            grid_product = list(ParameterGrid(grid))\n",
    "            rng = np.random.RandomState(random_state)\n",
    "            rng.shuffle(grid_product)\n",
    "            candidates.extend(grid_product[:max(1, stage2_max_candidates // max(1, top_k))])\n",
    "\n",
    "    # 去重\n",
    "    uniq = []\n",
    "    seen = set()\n",
    "    for d in candidates:\n",
    "        t = tuple(sorted(d.items()))\n",
    "        if t not in seen:\n",
    "            seen.add(t)\n",
    "            uniq.append(d)\n",
    "    if len(uniq) == 0:\n",
    "        uniq = [top.iloc[0]['params']]\n",
    "\n",
    "    # 将候选（标量）包装为 GridSearch 可接受的“列表网格”\n",
    "    param_grid_list = []\n",
    "    for d in uniq:\n",
    "        pg = {}\n",
    "        for k, v in d.items():\n",
    "            pg[k] = v if isinstance(v, (list, tuple, np.ndarray)) else [v]\n",
    "        param_grid_list.append(pg)\n",
    "\n",
    "    gs = GridSearchCV(pl, param_grid=param_grid_list, scoring='average_precision', cv=cv, n_jobs=-1, refit=True, verbose=0)\n",
    "    gs.fit(X, y)\n",
    "\n",
    "    best_pl = gs.best_estimator_\n",
    "    best_model = best_pl.named_steps['model']\n",
    "\n",
    "    print(f\"[{model_name}] Stage1 best AUPRC: {top.iloc[0]['mean_test_score']:.5f}; Stage2 best AUPRC: {gs.best_score_:.5f}\")\n",
    "    print(f\"[{model_name}] Best params (model):\", {k.split('__',1)[1]: v for k, v in gs.best_params_.items() if k.startswith('model__')})\n",
    "\n",
    "    # 早停微调迭代数（仅树模型），返回一个“未拟合”的新实例，后续由 Stacking 统一拟合\n",
    "    if enable_post_es and model_name in ('lgb', 'xgb', 'cat'):\n",
    "        tuned = _post_early_stopping_refit(model_name, best_model, X, y, random_state=random_state, stopping_rounds=50, use_focal=use_focal)\n",
    "        return tuned\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8f16f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lr1] Stage1 best AUPRC: 0.03010; Stage2 best AUPRC: 0.03039\n",
      "[lr1] Best params (model): {'C': 3.251294}\n",
      "[lr2] Stage1 best AUPRC: 0.04892; Stage2 best AUPRC: 0.05036\n",
      "[lr2] Best params (model): {'C': 3.052211}\n",
      "[lr2] Stage1 best AUPRC: 0.04892; Stage2 best AUPRC: 0.05036\n",
      "[lr2] Best params (model): {'C': 3.052211}\n",
      "[cat] Stage1 best AUPRC: 0.09417; Stage2 best AUPRC: 0.13939\n",
      "[cat] Best params (model): {'depth': 8, 'iterations': 402, 'l2_leaf_reg': 1.764162, 'learning_rate': 0.214575}\n",
      "[cat] Stage1 best AUPRC: 0.09417; Stage2 best AUPRC: 0.13939\n",
      "[cat] Best params (model): {'depth': 8, 'iterations': 402, 'l2_leaf_reg': 1.764162, 'learning_rate': 0.214575}\n",
      "[xgb] Stage1 best AUPRC: 0.14525; Stage2 best AUPRC: 0.16002\n",
      "[xgb] Best params (model): {'colsample_bytree': 1.0, 'gamma': 0.054915, 'learning_rate': 0.248668, 'max_depth': 4, 'min_child_weight': 7, 'n_estimators': 607, 'reg_alpha': 0.006215, 'reg_lambda': 3.242519, 'subsample': 1.0}\n",
      "\n",
      "已完成所有两阶段调参。\n",
      "[xgb] Stage1 best AUPRC: 0.14525; Stage2 best AUPRC: 0.16002\n",
      "[xgb] Best params (model): {'colsample_bytree': 1.0, 'gamma': 0.054915, 'learning_rate': 0.248668, 'max_depth': 4, 'min_child_weight': 7, 'n_estimators': 607, 'reg_alpha': 0.006215, 'reg_lambda': 3.242519, 'subsample': 1.0}\n",
      "\n",
      "已完成所有两阶段调参。\n"
     ]
    }
   ],
   "source": [
    "# # 运行两阶段调参：为 LR、CatBoost、LightGBM、XGBoost 以及 meta LR 分别调参\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# random_state = 20050520\n",
    "\n",
    "# # 基础模型（给出合理初值，便于 Stage-2 邻域生成）\n",
    "# lr1_base = LogisticRegression(solver='liblinear', penalty='l2', C=1.0, max_iter=10000, random_state=random_state)\n",
    "# lr2_base = LogisticRegression(solver='liblinear', penalty='l2', C=0.2, max_iter=10000, random_state=random_state*2)\n",
    "# cat_base = CatBoostClassifier(verbose=0, random_state=random_state, loss_function='Logloss', eval_metric='AUC')\n",
    "# lgb_base = LGBMClassifier(random_state=random_state, n_jobs=-1)\n",
    "# xgb_base = XGBClassifier(\n",
    "#     random_state=random_state,\n",
    "#     n_estimators=300,\n",
    "#     learning_rate=0.1,\n",
    "#     max_depth=6,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     n_jobs=-1,\n",
    "#     tree_method='hist',\n",
    "#     reg_lambda=1.0,\n",
    "#     reg_alpha=0.0,\n",
    "#     eval_metric='aucpr',\n",
    "#     gamma=0.0\n",
    "# )\n",
    "# meta_base = LogisticRegression(solver='liblinear', penalty='l2', C=0.5, max_iter=2000, random_state=random_state)\n",
    "\n",
    "# # 两阶段调参（为控制时间：LR 用 20 次，树模型 10 次；top_k=3，stage2 候选约 15 条）\n",
    "# lr1_best = tune_model_two_stage('lr1', lr1_base, X, y, random_state=random_state, stage1_iter=20, top_k=3, stage2_max_candidates=24)\n",
    "# lr2_best = tune_model_two_stage('lr2', lr2_base, X, y, random_state=random_state*2, stage1_iter=20, top_k=3, stage2_max_candidates=24)\n",
    "# cat_best = tune_model_two_stage('cat', cat_base, X, y, random_state=random_state, stage1_iter=10, top_k=5, stage2_max_candidates=15)\n",
    "# lgb_best = lgb_base\n",
    "# xgb_best = tune_model_two_stage('xgb', xgb_base, X, y, random_state=random_state, stage1_iter=10, top_k=5, stage2_max_candidates=15)\n",
    "# meta_best = meta_base\n",
    "# print('\\n已完成所有两阶段调参。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cc4143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "random_state = 20050520\n",
    "\n",
    "\n",
    "lr1_base = LogisticRegression(solver='liblinear', penalty='l2', C=3.251294, max_iter=10000, random_state=random_state)\n",
    "lr2_base = LogisticRegression(solver='liblinear', penalty='l2', C=3.052211, max_iter=10000, random_state=random_state*2)\n",
    "cat_base = CatBoostClassifier(verbose=0, random_state=random_state, loss_function='Logloss', eval_metric='AUC')\n",
    "lgb_base = LGBMClassifier(random_state=random_state, n_jobs=-1)\n",
    "xgb_base = XGBClassifier(\n",
    "    random_state=random_state,\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    n_jobs=-1,\n",
    "    tree_method='hist',\n",
    "    reg_lambda=1.0,\n",
    "    reg_alpha=0.0,\n",
    "    eval_metric='aucpr',\n",
    "    gamma=0.0\n",
    ")\n",
    "meta_base = LogisticRegression(solver='liblinear', penalty='l2', C=0.5, max_iter=2000, random_state=random_state)\n",
    "\n",
    "lr1_best = lr1_base\n",
    "lr2_best = lr2_base\n",
    "cat_best = cat_base\n",
    "lgb_best = lgb_base\n",
    "xgb_best = xgb_base\n",
    "meta_best = meta_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c964f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF stacking pipeline constructed. Bagging + nested-calibration will be used for base learners.\n"
     ]
    }
   ],
   "source": [
    "# 定义基学习器与 OOF 校准型 stacking（实现：先对每个已调参基学习器做 Bagging，然后用嵌套 CV + CalibratedClassifierCV 得到校准的 OOF 概率作为 meta 特征）\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline as SkPipeline\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 为 LR 单独加缩放管道并在管道内做折内 SMOTE；树模型使用类权重/scale_pos_weight（已在调参函数中设置）\n",
    "lr1 = ImbPipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTE(sampling_strategy=0.25, random_state=42)),\n",
    "    ('model', lr1_best)\n",
    "])\n",
    "lr2 = ImbPipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTE(sampling_strategy=0.25, random_state=42)),\n",
    "    ('model', lr2_best)\n",
    "])\n",
    "cat = cat_best  # CatBoost（已在调参函数中设置数值 class_weights）\n",
    "lgb = lgb_best  # LightGBM（已在调参函数中设置 class_weight/scale_pos_weight）\n",
    "xgb = xgb_best  # XGBoost\n",
    "\n",
    "# Meta 学习器（使用调好的 meta_base）\n",
    "meta = meta_best\n",
    "\n",
    "# ----- OOFStackingClassifier: 对外提供 sklearn 风格的 fit / predict_proba 接口，内部做 bagging + OOF 校准 + meta 训练\n",
    "class OOFStackingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_estimators, meta_estimator, n_bag=5, bag_kwargs=None, outer_cv=5, inner_calib_cv=3, random_state=42, n_jobs=1, verbose=1):\n",
    "        self.base_estimators = base_estimators\n",
    "        self.meta_estimator = meta_estimator\n",
    "        self.n_bag = n_bag\n",
    "        self.bag_kwargs = bag_kwargs or {}\n",
    "        self.outer_cv = outer_cv if isinstance(outer_cv, (int,)) else outer_cv\n",
    "        self.inner_calib_cv = inner_calib_cv\n",
    "        self.random_state = random_state\n",
    "        self.n_jobs = n_jobs\n",
    "        self.verbose = verbose\n",
    "        self.fitted_bases_ = {}\n",
    "\n",
    "    def _build_bag(self, estimator):\n",
    "        # Bagging 的 base_estimator 需要是未拟合的实例或可 clone 的对象\n",
    "        try:\n",
    "            return BaggingClassifier(base_estimator=estimator, n_estimators=self.n_bag, n_jobs=self.n_jobs, random_state=self.random_state, **self.bag_kwargs)\n",
    "        except Exception:\n",
    "            # 尝试 clone 后再传入（兼容某些 pipeline/复杂 estimator）\n",
    "            return BaggingClassifier(base_estimator=clone(estimator), n_estimators=self.n_bag, n_jobs=self.n_jobs, random_state=self.random_state, **self.bag_kwargs)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # outer_cv 可以传入 int 或者 CV 对象\n",
    "        if isinstance(self.outer_cv, int):\n",
    "            outer = StratifiedKFold(n_splits=self.outer_cv, shuffle=True, random_state=self.random_state)\n",
    "        else:\n",
    "            outer = self.outer_cv\n",
    "\n",
    "        oof_preds = pd.DataFrame(index=np.arange(len(y)))\n",
    "        # 对每个基学习器，先构造 Bagging + Calibrated wrapper（用于在 cross_val_predict 中得到校准的 OOF 概率）\n",
    "        for name, est in self.base_estimators.items():\n",
    "            if self.verbose:\n",
    "                print(f'Processing base estimator: {name}')\n",
    "            bag = self._build_bag(est)\n",
    "            # CalibratedClassifierCV 中的 cv 用于在每个外层 fold 中做内部校准（即嵌套 CV），从而得到校准的 OOF 概率\n",
    "            calib = CalibratedClassifierCV(estimator=bag, method='sigmoid', cv=self.inner_calib_cv)\n",
    "            # cross_val_predict 将在 outer 的每个训练子集上拟合 calib（包含内部校准），并对验证子集输出概率 -> OOF 校准概率\n",
    "            try:\n",
    "                oof = cross_val_predict(calib, X, y, cv=outer, method='predict_proba', n_jobs=self.n_jobs)[:, 1]\n",
    "            except Exception as e:\n",
    "                # 某些 estimator + pipeline 组合在 n_jobs=-1 下会出现序列化问题，回退到单线程\n",
    "                if self.verbose:\n",
    "                    print(f'cross_val_predict failed with n_jobs={self.n_jobs} for {name}, retrying with n_jobs=1; error:', e)\n",
    "                oof = cross_val_predict(calib, X, y, cv=outer, method='predict_proba', n_jobs=1)[:, 1]\n",
    "            oof_preds[name] = oof\n",
    "            # 最后用全量训练数据拟合一次带校准的基学习器，用于对测试集做预测\n",
    "            if self.verbose:\n",
    "                print(f'Fitting final calibrated bag for {name} on full data...')\n",
    "            calib.fit(X, y)\n",
    "            self.fitted_bases_[name] = calib\n",
    "\n",
    "        # 用 OOF 概率作为 meta 特征训练 meta_estimator\n",
    "        meta_X = oof_preds.values\n",
    "        self.meta_estimator_ = clone(self.meta_estimator)\n",
    "        self.meta_estimator_.fit(meta_X, y)\n",
    "        if self.verbose:\n",
    "            print('Meta estimator trained on OOF calibrated probabilities.')\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # 对每个已拟合的基学习器取 predict_proba(X)[:,1] 构造 meta 特征矩阵\n",
    "        cols = []\n",
    "        names = list(self.fitted_bases_.keys())\n",
    "        for name in names:\n",
    "            proba = self.fitted_bases_[name].predict_proba(X)[:, 1]\n",
    "            cols.append(proba)\n",
    "        meta_X = np.vstack(cols).T if len(cols) > 0 else np.zeros((X.shape[0], 0))\n",
    "        # meta 输出概率\n",
    "        probs = self.meta_estimator_.predict_proba(meta_X)\n",
    "        return probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X)[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "# 构造基学习器字典（这里对每个已调参模型做 bagging，再由 OOFStackingClassifier 做嵌套校准）\n",
    "base_estimators = {\n",
    "    'lr1': lr1,\n",
    "    'lr2': lr2,\n",
    "    'cat': cat,\n",
    "    'lgb': lgb,\n",
    "    'xgb': xgb\n",
    "}\n",
    "\n",
    "# 实例化 OOF stacking（可调参数：n_bag, outer_cv, inner_calib_cv, n_jobs）\n",
    "stack = OOFStackingClassifier(base_estimators=base_estimators, meta_estimator=meta, n_bag=5, outer_cv=5, inner_calib_cv=3, random_state=42, n_jobs=1, verbose=1)\n",
    "\n",
    "# 顶层流水线：仅做缺失值填充（不在顶层做 SMOTE），OOFStackingClassifier 内部负责基学习器的预处理与 bagging 行为\n",
    "pipeline = SkPipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('model', stack)\n",
    "])\n",
    "\n",
    "print('OOF stacking pipeline constructed. Bagging + nested-calibration will be used for base learners.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48f38c56",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot clone object OOFStackingClassifier(bag_kwargs={},\n                      base_estimators={'cat': <catboost.core.CatBoostClassifier object at 0x000002383D4178F0>,\n                                       'lgb': LGBMClassifier(n_jobs=-1,\n                                                             random_state=20050520),\n                                       'lr1': Pipeline(steps=[('imputer',\n                                                               SimpleImputer(strategy='median')),\n                                                              ('scaler',\n                                                               StandardScaler()),\n                                                              ('smote',\n                                                               SMOTE(random_state=42,\n                                                                     sampling_strategy=0.25)),\n                                                              ('model',\n                                                               LogisticRegressi...\n                                                            learning_rate=0.248668,\n                                                            max_bin=None,\n                                                            max_cat_threshold=None,\n                                                            max_cat_to_onehot=None,\n                                                            max_delta_step=None,\n                                                            max_depth=4,\n                                                            max_leaves=None,\n                                                            min_child_weight=7,\n                                                            missing=nan,\n                                                            monotone_constraints=None,\n                                                            multi_strategy=None,\n                                                            n_estimators=607,\n                                                            n_jobs=-1,\n                                                            num_parallel_tree=None, ...)},\n                      meta_estimator=LogisticRegression(C=0.5, max_iter=2000,\n                                                        random_state=20050520,\n                                                        solver='liblinear')), as the constructor either does not set or modifies parameter bag_kwargs",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 交叉验证评估（AUPRC）\u001b[39;00m\n\u001b[32m      2\u001b[39m cv = StratifiedKFold(n_splits=\u001b[32m5\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m scores = \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maverage_precision\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mCV AUPRC scores:\u001b[39m\u001b[33m'\u001b[39m, scores)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mMean AUPRC: \u001b[39m\u001b[38;5;132;01m%.6f\u001b[39;00m\u001b[33m (+/- \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m % (scores.mean(), scores.std()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Competition\\数科统模\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Competition\\数科统模\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:677\u001b[39m, in \u001b[36mcross_val_score\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[32m    675\u001b[39m scorer = check_scoring(estimator, scoring=scoring)\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m cv_results = \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[33m\"\u001b[39m\u001b[33mtest_score\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Competition\\数科统模\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Competition\\数科统模\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:399\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[32m    398\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m results = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    419\u001b[39m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[32m    421\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    423\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Competition\\数科统模\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Competition\\数科统模\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Competition\\数科统模\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1911\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1908\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1910\u001b[39m \u001b[38;5;66;03m# Sequentially call the tasks and yield the results.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_dispatched_batches\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m   1913\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_dispatched_tasks\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Competition\\数科统模\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:80\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = \u001b[43m(\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_with_config_and_warning_filters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarning_filters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdelayed_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(iterable_with_config_and_warning_filters)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Competition\\数科统模\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:401\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[32m    398\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m    399\u001b[39m results = parallel(\n\u001b[32m    400\u001b[39m     delayed(_fit_and_score)(\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    402\u001b[39m         X,\n\u001b[32m    403\u001b[39m         y,\n\u001b[32m    404\u001b[39m         scorer=scorers,\n\u001b[32m    405\u001b[39m         train=train,\n\u001b[32m    406\u001b[39m         test=test,\n\u001b[32m    407\u001b[39m         verbose=verbose,\n\u001b[32m    408\u001b[39m         parameters=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    409\u001b[39m         fit_params=routed_params.estimator.fit,\n\u001b[32m    410\u001b[39m         score_params=routed_params.scorer.score,\n\u001b[32m    411\u001b[39m         return_train_score=return_train_score,\n\u001b[32m    412\u001b[39m         return_times=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    413\u001b[39m         return_estimator=return_estimator,\n\u001b[32m    414\u001b[39m         error_score=error_score,\n\u001b[32m    415\u001b[39m     )\n\u001b[32m    416\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[32m    417\u001b[39m )\n\u001b[32m    419\u001b[39m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[32m    421\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    423\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Competition\\数科统模\\.venv\\Lib\\site-packages\\sklearn\\base.py:94\u001b[39m, in \u001b[36mclone\u001b[39m\u001b[34m(estimator, safe)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Construct a new unfitted estimator with the same parameters.\u001b[39;00m\n\u001b[32m     46\u001b[39m \n\u001b[32m     47\u001b[39m \u001b[33;03mClone does a deep copy of the model in an estimator\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     91\u001b[39m \u001b[33;03mFalse\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[33m\"\u001b[39m\u001b[33m__sklearn_clone__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect.isclass(estimator):\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__sklearn_clone__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _clone_parametrized(estimator, safe=safe)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Competition\\数科统模\\.venv\\Lib\\site-packages\\sklearn\\base.py:362\u001b[39m, in \u001b[36mBaseEstimator.__sklearn_clone__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__sklearn_clone__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_clone_parametrized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Competition\\数科统模\\.venv\\Lib\\site-packages\\sklearn\\base.py:127\u001b[39m, in \u001b[36m_clone_parametrized\u001b[39m\u001b[34m(estimator, safe)\u001b[39m\n\u001b[32m    125\u001b[39m new_object_params = estimator.get_params(deep=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m new_object_params.items():\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     new_object_params[name] = \u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m new_object = klass(**new_object_params)\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Competition\\数科统模\\.venv\\Lib\\site-packages\\sklearn\\base.py:95\u001b[39m, in \u001b[36mclone\u001b[39m\u001b[34m(estimator, safe)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[33m\"\u001b[39m\u001b[33m__sklearn_clone__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect.isclass(estimator):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m estimator.__sklearn_clone__()\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_clone_parametrized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafe\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Competition\\数科统模\\.venv\\Lib\\site-packages\\sklearn\\base.py:105\u001b[39m, in \u001b[36m_clone_parametrized\u001b[39m\u001b[34m(estimator, safe)\u001b[39m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: clone(v, safe=safe) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m estimator.items()}\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m estimator_type \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mset\u001b[39m, \u001b[38;5;28mfrozenset\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m estimator_type([\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafe\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m estimator])\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[33m\"\u001b[39m\u001b[33mget_params\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(estimator, \u001b[38;5;28mtype\u001b[39m):\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m safe:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Competition\\数科统模\\.venv\\Lib\\site-packages\\sklearn\\base.py:95\u001b[39m, in \u001b[36mclone\u001b[39m\u001b[34m(estimator, safe)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[33m\"\u001b[39m\u001b[33m__sklearn_clone__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect.isclass(estimator):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m estimator.__sklearn_clone__()\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_clone_parametrized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafe\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Competition\\数科统模\\.venv\\Lib\\site-packages\\sklearn\\base.py:105\u001b[39m, in \u001b[36m_clone_parametrized\u001b[39m\u001b[34m(estimator, safe)\u001b[39m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: clone(v, safe=safe) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m estimator.items()}\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m estimator_type \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mset\u001b[39m, \u001b[38;5;28mfrozenset\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m estimator_type([\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafe\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m estimator])\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[33m\"\u001b[39m\u001b[33mget_params\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(estimator, \u001b[38;5;28mtype\u001b[39m):\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m safe:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Competition\\数科统模\\.venv\\Lib\\site-packages\\sklearn\\base.py:94\u001b[39m, in \u001b[36mclone\u001b[39m\u001b[34m(estimator, safe)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Construct a new unfitted estimator with the same parameters.\u001b[39;00m\n\u001b[32m     46\u001b[39m \n\u001b[32m     47\u001b[39m \u001b[33;03mClone does a deep copy of the model in an estimator\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     91\u001b[39m \u001b[33;03mFalse\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[33m\"\u001b[39m\u001b[33m__sklearn_clone__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect.isclass(estimator):\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__sklearn_clone__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _clone_parametrized(estimator, safe=safe)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Competition\\数科统模\\.venv\\Lib\\site-packages\\sklearn\\base.py:362\u001b[39m, in \u001b[36mBaseEstimator.__sklearn_clone__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__sklearn_clone__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_clone_parametrized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Competition\\数科统模\\.venv\\Lib\\site-packages\\sklearn\\base.py:142\u001b[39m, in \u001b[36m_clone_parametrized\u001b[39m\u001b[34m(estimator, safe)\u001b[39m\n\u001b[32m    140\u001b[39m     param2 = params_set[name]\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m param1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m param2:\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    143\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCannot clone object \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, as the constructor \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    144\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33meither does not set or modifies parameter \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (estimator, name)\n\u001b[32m    145\u001b[39m         )\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m# _sklearn_output_config is used by `set_output` to configure the output\u001b[39;00m\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# container of an estimator.\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[33m\"\u001b[39m\u001b[33m_sklearn_output_config\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mRuntimeError\u001b[39m: Cannot clone object OOFStackingClassifier(bag_kwargs={},\n                      base_estimators={'cat': <catboost.core.CatBoostClassifier object at 0x000002383D4178F0>,\n                                       'lgb': LGBMClassifier(n_jobs=-1,\n                                                             random_state=20050520),\n                                       'lr1': Pipeline(steps=[('imputer',\n                                                               SimpleImputer(strategy='median')),\n                                                              ('scaler',\n                                                               StandardScaler()),\n                                                              ('smote',\n                                                               SMOTE(random_state=42,\n                                                                     sampling_strategy=0.25)),\n                                                              ('model',\n                                                               LogisticRegressi...\n                                                            learning_rate=0.248668,\n                                                            max_bin=None,\n                                                            max_cat_threshold=None,\n                                                            max_cat_to_onehot=None,\n                                                            max_delta_step=None,\n                                                            max_depth=4,\n                                                            max_leaves=None,\n                                                            min_child_weight=7,\n                                                            missing=nan,\n                                                            monotone_constraints=None,\n                                                            multi_strategy=None,\n                                                            n_estimators=607,\n                                                            n_jobs=-1,\n                                                            num_parallel_tree=None, ...)},\n                      meta_estimator=LogisticRegression(C=0.5, max_iter=2000,\n                                                        random_state=20050520,\n                                                        solver='liblinear')), as the constructor either does not set or modifies parameter bag_kwargs"
     ]
    }
   ],
   "source": [
    "# 交叉验证评估（AUPRC）\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(pipeline, X, y, cv=cv, scoring='average_precision', n_jobs=1)\n",
    "print('CV AUPRC scores:', scores)\n",
    "print('Mean AUPRC: %.6f (+/- %.4f)' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1c42af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission to d:\\Competition\\数科统模\\submit\\（adjust）stacking_logit_meta_logitC05_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# 在全部训练数据上训练并对测试集做预测（概率）\n",
    "pipeline.fit(X, y)\n",
    "# 注意：pipeline 的顶层 model 名称为 'model'，之前为 'stack'，因此检查 model 的 predict_proba\n",
    "if hasattr(pipeline, 'predict_proba') or ('model' in pipeline.named_steps and hasattr(pipeline.named_steps['model'], 'predict_proba')):\n",
    "    proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "else:\n",
    "    # fallback: decision_function -> scale to [0,1] via sigmoid\n",
    "    from scipy.special import expit\n",
    "    dec = pipeline.decision_function(X_test)\n",
    "    proba = expit(dec)\n",
    "\n",
    "# 构建提交文件：优先使用 test 中的原始 id 列，避免顺序或重采样带来的潜在错位\n",
    "id_col = 'id' if 'id' in test.columns else ('ID' if 'ID' in test.columns else None)\n",
    "if id_col is not None:\n",
    "    submission = pd.DataFrame({'id': test[id_col].values, 'target': proba})\n",
    "else:\n",
    "    # 兜底：若数据不含 id 列，按题面从 501 连续编号\n",
    "    submission = pd.DataFrame({'id': 501+np.arange(len(proba)), 'target': proba})\n",
    "\n",
    "out_dir = os.path.join(ROOT, 'submit')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_path = os.path.join(out_dir, '（adjust3）stacking_logit_meta_logitC05_submission.csv')\n",
    "# submission.to_csv(out_path, index=False)\n",
    "print('Saved submission to', out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-competition-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
